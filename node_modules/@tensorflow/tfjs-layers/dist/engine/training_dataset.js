"use strict";
/**
 * @license
 * Copyright 2018 Google LLC
 *
 * Use of this source code is governed by an MIT-style
 * license that can be found in the LICENSE file or at
 * https://opensource.org/licenses/MIT.
 * =============================================================================
 */
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __generator = (this && this.__generator) || function (thisArg, body) {
    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;
    return g = { next: verb(0), "throw": verb(1), "return": verb(2) }, typeof Symbol === "function" && (g[Symbol.iterator] = function() { return this; }), g;
    function verb(n) { return function (v) { return step([n, v]); }; }
    function step(op) {
        if (f) throw new TypeError("Generator is already executing.");
        while (_) try {
            if (f = 1, y && (t = op[0] & 2 ? y["return"] : op[0] ? y["throw"] || ((t = y["return"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;
            if (y = 0, t) op = [op[0] & 2, t.value];
            switch (op[0]) {
                case 0: case 1: t = op; break;
                case 4: _.label++; return { value: op[1], done: false };
                case 5: _.label++; y = op[1]; op = [0]; continue;
                case 7: op = _.ops.pop(); _.trys.pop(); continue;
                default:
                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }
                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }
                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }
                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }
                    if (t[2]) _.ops.pop();
                    _.trys.pop(); continue;
            }
            op = body.call(thisArg, _);
        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }
        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };
    }
};
Object.defineProperty(exports, "__esModule", { value: true });
/**
 * Interfaces and methods for training models using TensorFlow.js datasets.
 */
var tfc = require("@tensorflow/tfjs-core");
var tfjs_core_1 = require("@tensorflow/tfjs-core");
var base_callbacks_1 = require("../base_callbacks");
var errors_1 = require("../errors");
var logs_1 = require("../logs");
var generic_utils_1 = require("../utils/generic_utils");
var training_utils_1 = require("./training_utils");
// Default batch size used during tensor-based validation.
var DEFAULT_VALIDATION_BATCH_SIZE = 32;
/**
 * Standardize the output of a dataset iterator for use by
 * LayersModel.fitDataset().
 *
 * @param model: A `tf.LayersModel` object.
 * @param iteratorOut The output of a dataset iterator. It is required to be
 *   an object of the form `{xs: TensorOrArrayOrMap, ys:
 * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,
 * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.
 * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s
 *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided
 *   as a map, the order in the resulting array is taken from the `inputNames`
 *   and `outputNames` of the model.
 */
function standardizeDataIteratorOutput(
// Type `model` as `any` here to avoid circular dependency w/
// training.ts.
// tslint:disable-next-line:no-any
model, iteratorOut) {
    var xs;
    var ys;
    var iteratorOutObj = iteratorOut;
    xs = iteratorOutObj['xs'];
    ys = iteratorOutObj['ys'];
    tfc.util.assert(xs != null && ys != null, function () { return 'A Dataset iterator for fitDataset() is expected to generate ' +
        'objects of the form `{xs: xVal, ys: yVal}`, where the two ' +
        'values may be `tf.Tensor`, an array of Tensors, or a map of ' +
        'string to Tensor.  The provided Dataset instead generates ' +
        ("" + iteratorOut); });
    var flattenedXs = flattenTensorOrArrayOrMap('input', model.inputNames, xs);
    var flattenedYs = flattenTensorOrArrayOrMap('output', model.outputNames, ys);
    var batchSize = flattenedXs[0].shape[0];
    tfc.util.assert(flattenedXs.length === model.inputs.length, function () { return "LayersModel has " + model.inputs.length + " inputs, but the dataset " +
        ("provides " + flattenedXs.length + " inputs.  (Expected input keys: ") +
        (JSON.stringify(model.inputNames) + ")"); });
    tfc.util.assert(flattenedYs.length === model.outputs.length, function () {
        return "LayersModel has " + model.outputs.length + " outputs, but the dataset " +
            ("provides " + flattenedYs.length + " outputs.  (Expected output keys: ") +
            (JSON.stringify(model.outputNames) + ")");
    });
    var _loop_1 = function (xIndex) {
        tfc.util.assert(flattenedXs[xIndex].shape[0] === batchSize, function () { return "Batch size mismatch: input " +
            (model.inputNames[xIndex] + " has " + flattenedXs[xIndex].shape[0] + "; ") +
            ("expected  " + batchSize + " based on input " + model.inputNames[0] + "."); });
    };
    for (var xIndex = 0; xIndex < flattenedXs.length; xIndex++) {
        _loop_1(xIndex);
    }
    var _loop_2 = function (yIndex) {
        tfc.util.assert(flattenedYs[yIndex].shape[0] === batchSize, function () { return "Batch size mismatch: output " +
            (model.outputNames[yIndex] + " has " + flattenedYs[yIndex].shape[0] + "; ") +
            ("expected  " + batchSize + " based on input " + model.inputNames[0] + "."); });
    };
    for (var yIndex = 0; yIndex < flattenedYs.length; yIndex++) {
        _loop_2(yIndex);
    }
    return { xs: flattenedXs, ys: flattenedYs };
}
function flattenTensorOrArrayOrMap(inputOrOutput, names, values) {
    if (values instanceof tfc.Tensor) {
        return [values];
    }
    else if (Array.isArray(values)) {
        tfc.util.assert(values.length === names.length, function () { return "Received an array of " + values.length + " Tensors, but expected " + names.length + " to match the " + inputOrOutput + " keys " + names + "."; });
        return values;
    }
    else {
        var result = [];
        // Check that all the required keys are available.
        for (var _i = 0, names_1 = names; _i < names_1.length; _i++) {
            var name_1 = names_1[_i];
            if (values[name_1] == null) {
                throw new errors_1.ValueError("The feature data generated by the dataset lacks the required " +
                    (inputOrOutput + " key '" + name_1 + "'."));
            }
            result.push(values[name_1]);
        }
        return result;
    }
}
function standardizeTensorValidationData(data) {
    if (data.length === 3) {
        throw new errors_1.NotImplementedError('Validation with sample weights is not implemented yet.');
    }
    return { xs: data[0], ys: data[1] };
}
function fitDataset(
// Type `model` as `any` here to avoid circular dependency w/
// training.ts.
// tslint:disable-next-line:no-any
model, dataset, args) {
    return __awaiter(this, void 0, void 0, function () {
        var hasBatchesPerEpoch, doValidation, valXs, valYs, validationData, trainFunction, outLabels, callbackMetrics, callbacks, verbose, _a, callbackList, history_1, epoch, dataIterator, epochLogs, stepsDone, batchIndex, iteratorOut, _b, xs, ys, batchLogs, sampleWeights, standardClassWeights, i, _c, _d, ins, outs, i, label, out, valOuts, _e, i;
        return __generator(this, function (_f) {
            switch (_f.label) {
                case 0:
                    hasBatchesPerEpoch = args.batchesPerEpoch != null;
                    tfc.util.assert(model.optimizer != null, function () { return 'You must compile a model before training/testing. Use ' +
                        'LayersModel.compile(modelCompileConfig).'; });
                    tfc.util.assert(args != null, function () { return "For fitDataset(), the 2nd argument (config) is required, " +
                        "but it is not provided in this call."; });
                    tfc.util.assert(args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs), function () { return "For fitDataset(), config.epochs is expected to be a positive " +
                        ("integer, but got " + args.epochs); });
                    tfc.util.assert(!hasBatchesPerEpoch ||
                        (args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch)), function () { return "For fitDataset(), config.batchesPerEpoch is expected to be a " +
                        ("positive integer if specified, but got " + args.batchesPerEpoch); });
                    tfc.util.assert(
                    // tslint:disable-next-line:no-any
                    args['validationSplit'] == null, function () { return '`validationSplit` is not supported by `fitDataset()`. ' +
                        'Use validationData instead.'; });
                    if (model.isTraining) {
                        throw new Error('Cannot start training because another fit() call is ongoing.');
                    }
                    model.isTraining = true;
                    _f.label = 1;
                case 1:
                    _f.trys.push([1, , 26, 27]);
                    doValidation = args.validationData != null;
                    valXs = void 0;
                    valYs = void 0;
                    if (doValidation) {
                        if (isDatasetObject(args.validationData)) {
                            tfc.util.assert(args.validationBatches == null ||
                                (args.validationBatches > 0 &&
                                    Number.isInteger(args.validationBatches)), function () { return "For fitDataset() with dataset-based validation, " +
                                "config.validationBatches is expected not to be provided, " +
                                "or to be a positive integer, " +
                                ("but got " + args.validationBatches); });
                        }
                        else {
                            validationData = standardizeTensorValidationData(args.validationData);
                            valXs = validationData.xs;
                            valYs = validationData.ys;
                        }
                    }
                    trainFunction = model.makeTrainFunction();
                    outLabels = model.getDedupedMetricsNames();
                    callbackMetrics = void 0;
                    if (doValidation) {
                        callbackMetrics =
                            outLabels.slice().concat(outLabels.map(function (n) { return 'val_' + n; }));
                    }
                    else {
                        callbackMetrics = outLabels.slice();
                    }
                    callbacks = base_callbacks_1.standardizeCallbacks(args.callbacks, args.yieldEvery);
                    verbose = args.verbose == null ? 1 : args.verbose;
                    _a = base_callbacks_1.configureCallbacks(callbacks, verbose, args.epochs, null, null, getStepsPerEpoch(dataset, args), null, // Batch size determined by the dataset itself.
                    doValidation, callbackMetrics), callbackList = _a.callbackList, history_1 = _a.history;
                    callbackList.setModel(model);
                    model.history = history_1;
                    return [4 /*yield*/, callbackList.onTrainBegin()];
                case 2:
                    _f.sent();
                    model.stopTraining_ = false;
                    epoch = args.initialEpoch == null ? 0 : args.initialEpoch;
                    return [4 /*yield*/, dataset.iterator()];
                case 3:
                    dataIterator = _f.sent();
                    _f.label = 4;
                case 4:
                    if (!(epoch < args.epochs)) return [3 /*break*/, 23];
                    epochLogs = {};
                    return [4 /*yield*/, callbackList.onEpochBegin(epoch)];
                case 5:
                    _f.sent();
                    stepsDone = 0;
                    batchIndex = 0;
                    if (!!hasBatchesPerEpoch) return [3 /*break*/, 7];
                    return [4 /*yield*/, dataset.iterator()];
                case 6:
                    dataIterator = _f.sent();
                    _f.label = 7;
                case 7:
                    if (!(hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true)) return [3 /*break*/, 21];
                    return [4 /*yield*/, dataIterator.next()];
                case 8:
                    iteratorOut = _f.sent();
                    // If `batchesPerEpoch` is specified, the dataset should not be
                    // exhausted until all epoches are done.
                    if (hasBatchesPerEpoch && iteratorOut.done) {
                        console.warn('You provided `batchesPerEpoch` as ' +
                            (args.batchesPerEpoch + ", ") +
                            'but your dataset iterator ran out of data after ' +
                            (stepsDone + " batches; ") +
                            'interrupting training. Make sure that your ' +
                            'dataset can generate at least `batchesPerEpoch * epochs` ' +
                            'batches (in this case, ' +
                            (args.batchesPerEpoch * args.epochs + " batches). ") +
                            'You may need to use the repeat() function when building ' +
                            'your dataset.');
                        return [3 /*break*/, 21];
                    }
                    if (!(iteratorOut.value != null)) return [3 /*break*/, 15];
                    _b = standardizeDataIteratorOutput(model, iteratorOut.value), xs = _b.xs, ys = _b.ys;
                    batchLogs = {};
                    batchLogs['batch'] = batchIndex;
                    batchLogs['size'] = xs[0].shape[0];
                    return [4 /*yield*/, callbackList.onBatchBegin(batchIndex, batchLogs)];
                case 9:
                    _f.sent();
                    sampleWeights = [];
                    if (!(args.classWeight != null)) return [3 /*break*/, 13];
                    standardClassWeights = training_utils_1.standardizeClassWeights(args.classWeight, model.outputNames);
                    i = 0;
                    _f.label = 10;
                case 10:
                    if (!(i < standardClassWeights.length)) return [3 /*break*/, 13];
                    _d = (_c = sampleWeights).push;
                    return [4 /*yield*/, training_utils_1.standardizeWeights(ys[i], null, standardClassWeights[i])];
                case 11:
                    _d.apply(_c, [_f.sent()]);
                    _f.label = 12;
                case 12:
                    ++i;
                    return [3 /*break*/, 10];
                case 13:
                    ins = xs.concat(ys).concat(sampleWeights);
                    outs = trainFunction(ins);
                    tfc.dispose(ins);
                    for (i = 0; i < outLabels.length; ++i) {
                        label = outLabels[i];
                        out = outs[i];
                        batchLogs[label] = out;
                        tfc.keep(out);
                    }
                    return [4 /*yield*/, callbackList.onBatchEnd(batchIndex, batchLogs)];
                case 14:
                    _f.sent();
                    logs_1.disposeTensorsInLogs(batchLogs);
                    batchIndex++;
                    stepsDone++;
                    _f.label = 15;
                case 15:
                    if (!(hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch :
                        iteratorOut.done)) return [3 /*break*/, 20];
                    if (!doValidation) return [3 /*break*/, 19];
                    valOuts = void 0;
                    if (!isDatasetObject(args.validationData)) return [3 /*break*/, 17];
                    _e = generic_utils_1.toList;
                    return [4 /*yield*/, model.evaluateDataset(args.validationData, { batches: args.validationBatches })];
                case 16:
                    valOuts = _e.apply(void 0, [_f.sent()]);
                    return [3 /*break*/, 18];
                case 17:
                    valOuts = generic_utils_1.toList(model.evaluate(valXs, valYs, {
                        batchSize: args.validationBatchSize == null ?
                            DEFAULT_VALIDATION_BATCH_SIZE :
                            args.validationBatchSize,
                        verbose: 0
                    }));
                    _f.label = 18;
                case 18:
                    for (i = 0; i < model.metricsNames.length; ++i) {
                        epochLogs["val_" + model.metricsNames[i]] = valOuts[i];
                    }
                    _f.label = 19;
                case 19: 
                // Call `break` to exit one epoch lopp after validation is done. If
                // config.batchesPerEpoch is specified, an epoch while loop will
                // stop when `stepsDone >= config.batchesPerEpoch`. When
                // config.batchesPerEpoch is not provided, the following `break` is
                // required to exit the while lopp after dataset is exhausted.
                return [3 /*break*/, 21];
                case 20:
                    if (model.stopTraining_) {
                        return [3 /*break*/, 21];
                    }
                    return [3 /*break*/, 7];
                case 21: return [4 /*yield*/, callbackList.onEpochEnd(epoch, epochLogs)];
                case 22:
                    _f.sent();
                    epoch++;
                    if (model.stopTraining_) {
                        return [3 /*break*/, 23];
                    }
                    return [3 /*break*/, 4];
                case 23: return [4 /*yield*/, callbackList.onTrainEnd()];
                case 24:
                    _f.sent();
                    return [4 /*yield*/, model.history.syncData()];
                case 25:
                    _f.sent();
                    return [2 /*return*/, model.history];
                case 26:
                    model.isTraining = false;
                    return [7 /*endfinally*/];
                case 27: return [2 /*return*/];
            }
        });
    });
}
exports.fitDataset = fitDataset;
/** Helper function that determines number of steps (batches) per epoch. */
function getStepsPerEpoch(dataset, args) {
    // Attempt to determine # of batches in an epoch.
    var stepsPerEpoch = null;
    if (args.batchesPerEpoch != null) {
        stepsPerEpoch = args.batchesPerEpoch;
    }
    else if (Number.isFinite(dataset.size)) {
        stepsPerEpoch = dataset.size;
    }
    return stepsPerEpoch;
}
// Check if provided object is a Dataset object by checking it's .iterator
// element.
function isDatasetObject(dataset) {
    return (typeof dataset.iterator === 'function');
}
// Check if provided object is a LazyIterator object by checking it's .next
// element.
function isLazyIteratorObject(iterator) {
    return (typeof iterator.next === 'function');
}
function evaluateDataset(
// Type `model` as `any` here to avoid circular dependency w/
// training.ts.
// tslint:disable-next-line:no-any
model, dataset, args) {
    return __awaiter(this, void 0, void 0, function () {
        var hasBatches, f, outs, dataIterator, _a, numExamples, batch, _loop_3, state_1, i, oldScalar;
        return __generator(this, function (_b) {
            switch (_b.label) {
                case 0:
                    args = args || {};
                    hasBatches = args.batches != null;
                    f = model.testFunction;
                    outs = [];
                    if (args.verbose > 0) {
                        throw new errors_1.NotImplementedError('Verbose mode is not implemented yet.');
                    }
                    tfc.util.assert(!hasBatches || (args.batches > 0 && Number.isInteger(args.batches)), function () { return 'Test loop expects `batches` to be a positive integer, but ' +
                        ("received " + JSON.stringify(args.batches)); });
                    if (!isLazyIteratorObject(dataset)) return [3 /*break*/, 1];
                    _a = dataset;
                    return [3 /*break*/, 3];
                case 1: return [4 /*yield*/, dataset.iterator()];
                case 2:
                    _a = _b.sent();
                    _b.label = 3;
                case 3:
                    dataIterator = _a;
                    numExamples = 0;
                    batch = 0;
                    _loop_3 = function () {
                        var iteratorOut;
                        return __generator(this, function (_a) {
                            switch (_a.label) {
                                case 0: return [4 /*yield*/, dataIterator.next()];
                                case 1:
                                    iteratorOut = _a.sent();
                                    outs = tfc.tidy(function () {
                                        if (iteratorOut.value) {
                                            // TODO(cais): Once real dataset is available, use
                                            //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.
                                            var _a = standardizeDataIteratorOutput(model, iteratorOut.value), xs = _a.xs, ys = _a.ys;
                                            var xsAndYs_1 = xs.concat(ys);
                                            var batchOuts = tfc.tidy(function () { return f(xsAndYs_1); });
                                            tfc.dispose(xsAndYs_1);
                                            if (batch === 0) {
                                                for (var i = 0; i < batchOuts.length; ++i) {
                                                    outs.push(tfjs_core_1.scalar(0));
                                                }
                                            }
                                            var batchSize_1 = xsAndYs_1[0].shape[0];
                                            var _loop_4 = function (i) {
                                                var batchOut = batchOuts[i];
                                                var oldScalar = outs[i];
                                                outs[i] =
                                                    tfc.tidy(function () { return tfc.add(outs[i], tfc.mul(batchSize_1, batchOut)); });
                                                if (batch > 0) {
                                                    tfc.dispose(oldScalar);
                                                }
                                            };
                                            for (var i = 0; i < batchOuts.length; ++i) {
                                                _loop_4(i);
                                            }
                                            tfc.dispose(batchOuts);
                                            numExamples += batchSize_1;
                                            ++batch;
                                        }
                                        return outs;
                                    });
                                    if (iteratorOut.done) {
                                        if (hasBatches) {
                                            console.warn('Your dataset iterator ran out of data during evaluateDataset(). ' +
                                                'Interrupting evalution. Make sure that your ' +
                                                'dataset can generate at least `batches` ' +
                                                ("batches (in this case, " + args.batches + " batches). ") +
                                                'You may need to use the repeat() function when building ' +
                                                'your dataset.');
                                        }
                                        return [2 /*return*/, "break"];
                                    }
                                    return [2 /*return*/];
                            }
                        });
                    };
                    _b.label = 4;
                case 4:
                    if (!(hasBatches ? batch < args.batches : true)) return [3 /*break*/, 6];
                    return [5 /*yield**/, _loop_3()];
                case 5:
                    state_1 = _b.sent();
                    if (state_1 === "break")
                        return [3 /*break*/, 6];
                    return [3 /*break*/, 4];
                case 6:
                    for (i = 0; i < outs.length; ++i) {
                        oldScalar = outs[i];
                        outs[i] = tfc.div(outs[i], numExamples);
                        tfc.dispose(oldScalar);
                    }
                    return [2 /*return*/, generic_utils_1.singletonOrArray(outs)];
            }
        });
    });
}
exports.evaluateDataset = evaluateDataset;
//# sourceMappingURL=training_dataset.js.map