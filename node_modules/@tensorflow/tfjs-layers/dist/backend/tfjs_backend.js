"use strict";
/**
 * @license
 * Copyright 2018 Google LLC
 *
 * Use of this source code is governed by an MIT-style
 * license that can be found in the LICENSE file or at
 * https://opensource.org/licenses/MIT.
 * =============================================================================
 */
Object.defineProperty(exports, "__esModule", { value: true });
/**
 * deeplearn.js backend.
 */
var tfc = require("@tensorflow/tfjs-core");
var tfjs_core_1 = require("@tensorflow/tfjs-core");
var common_1 = require("../common");
var errors_1 = require("../errors");
var math_utils = require("../utils/math_utils");
var common_2 = require("./common");
// tslint:enable
/* Setting and getting backend from deeplearn.js. */
// Default deeplearn.js backend is WebGL (GPU).
var backend = 'webgl';
function setBackend(requestedBackend) {
    tfc.setBackend(requestedBackend);
    backend = requestedBackend;
}
exports.setBackend = setBackend;
function getBackend() {
    return backend;
}
exports.getBackend = getBackend;
/**
 * Indicates whether the backend is operating symbolically.
 *
 * This function will be used to determine how to interpret user code. If
 * it returns true, calls to the backend construct a symbolic graph; if
 * it returns false, calls to the backend execute immediately.
 */
function isBackendSymbolic() {
    return false;
}
exports.isBackendSymbolic = isBackendSymbolic;
/**
 * Get the number of elements in a Tensor.
 * @param x The Tensor.
 * @return Number of elements in `x`.
 */
function countParams(x) {
    var shape = x.shape;
    if (shape.length > 0) {
        return shape.reduce(function (a, b) { return a * b; });
    }
    else {
        // Scalar.
        return 1;
    }
}
exports.countParams = countParams;
/**
 * Casts a tensor to a different dtype and returns it.
 * @param x Input tensor.
 * @param dtype String: 'float32'|'int32'|'bool'.
 * @returns Tensor of the specified `dtype`.
 */
function cast(x, dtype) {
    return x.asType(dtype);
}
exports.cast = cast;
/**
 * Adds a 1-sized dimension at index "axis".
 * @param x Input tensor.
 * @param axis Position where to add the new axis.
 * @returns Result of the dimension expansion.
 */
function expandDims(x, axis) {
    if (axis === void 0) { axis = -1; }
    var outShape = x.shape.slice();
    if (axis < 0) {
        axis = outShape.length + axis + 1;
    }
    outShape.splice(axis, 0, 1);
    return x.reshape(outShape);
}
exports.expandDims = expandDims;
/**
 * Repeats a 2D tensor.
 *
 * If `x` has shape `[samples, dim]` and `n` is 2, for example, the output
 * will have shape `[samples, 2, dim]`.
 *
 * @param x Input tensor.
 * @param n Integer, number of times to repeat.
 * @returns The result of the repeat operation.
 * @throws ValueError: If input tensor is not 2D.
 */
function repeat(x, n) {
    return tfjs_core_1.tidy(function () {
        if (x.shape.length !== 2) {
            throw new errors_1.ValueError("repeat() expects a rank-2 tensor, but received a " +
                ("rank-" + x.shape.length + " tensor."));
        }
        var y = expandDims(x, 1);
        return tile(y, [1, n, 1]);
    });
}
exports.repeat = repeat;
/**
 * Flatten an Tensor into 1D.
 * @param x Input tensor.
 * @return The result of the flattening `x`.
 */
function flatten(x) {
    var newShape = [math_utils.arrayProd(x.shape)];
    return x.reshape(newShape);
}
exports.flatten = flatten;
/**
 * Turn a nD tensor into a 2D tensor with same 0th dimension.
 * In other words, it flattens each data samples of a batch.
 *
 * @param x The tensor to flatten. The rank of this tensor is required to be 2
 *   or higher.
 * @return The result of the flattening.
 */
function batchFlatten(x) {
    if (x.rank <= 1) {
        throw new errors_1.ValueError("batchFlatten requires a minimum rank of 2. Got rank: " + x.rank + ".");
    }
    var newShape = [x.shape[0], math_utils.arrayProd(x.shape, 1)];
    return x.reshape(newShape);
}
exports.batchFlatten = batchFlatten;
/**
 * Do slicing along the first axis.
 * @param array input `tf.Tensor`.
 * @param start starting index, inclusive.
 * @param size size of the slice along the first axis.
 * @returns result of the slicing.
 * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.
 */
function sliceAlongFirstAxis(array, start, size) {
    return tfjs_core_1.tidy(function () {
        switch (array.rank) {
            case 1:
                return tfc.slice1d(array, start, size);
            case 2:
                return tfc.slice2d(array, [start, 0], [size, array.shape[1]]);
            case 3:
                return tfc.slice3d(array, [start, 0, 0], [size, array.shape[1], array.shape[2]]);
            case 4:
                return tfc.slice4d(array, [start, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3]]);
            case 5:
                return tfc.slice(array, [start, 0, 0, 0, 0], [
                    size, array.shape[1], array.shape[2], array.shape[3], array.shape[4]
                ]);
            case 6:
                return tfc.slice(array, [start, 0, 0, 0, 0, 0], [
                    size, array.shape[1], array.shape[2], array.shape[3], array.shape[4],
                    array.shape[5]
                ]);
            default:
                throw new errors_1.ValueError("sliceAlongFirstAxis() received an unsupported tensor rank: " +
                    ("" + array.rank));
        }
    });
}
exports.sliceAlongFirstAxis = sliceAlongFirstAxis;
/**
 * Do slicing along the last axis.
 * @param array input `tf.Tensor`.
 * @param start starting index, inclusive.
 * @param size size of the slice along the last axis.
 * @returns result of the slicing.
 * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.
 */
function sliceAlongLastAxis(array, start, size) {
    return tfjs_core_1.tidy(function () {
        switch (array.rank) {
            case 1:
                return tfc.slice1d(array, start, size);
            case 2:
                return tfc.slice2d(array, [0, start], [array.shape[0], size]);
            case 3:
                return tfc.slice3d(array, [0, 0, start], [array.shape[0], array.shape[1], size]);
            case 4:
                return tfc.slice4d(array, [0, 0, 0, start], [array.shape[0], array.shape[1], array.shape[2], size]);
            default:
                throw new errors_1.ValueError("sliceAlongLastAxis() received an unsupported tensor rank: " +
                    ("" + array.rank));
        }
    });
}
exports.sliceAlongLastAxis = sliceAlongLastAxis;
/**
 * Do slicing along the sepcified axis.
 * @param array input `tf.Tensor`.
 * @param start starting index, inclusive.
 * @param size of the slice along the chosen axis.
 * @param choose an axis.
 * @returns result of the slicing.
 * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.
 */
function sliceAlongAxis(array, start, size, axis) {
    return tfjs_core_1.tidy(function () {
        switch (array.rank) {
            case 1:
                return tfc.slice1d(array, start, size);
            case 2:
                switch (axis) {
                    case 1:
                        return sliceAlongFirstAxis(array, start, size);
                    case 2:
                        return sliceAlongLastAxis(array, start, size);
                    default:
                        throw new errors_1.ValueError("The axis is not within the rank of the tensor " +
                            ("" + axis));
                }
            case 3:
                switch (axis) {
                    case 1:
                        return sliceAlongFirstAxis(array, start, size);
                    case 2:
                        return tfc.slice3d(array, [0, start, 0], [array.shape[0], size, array.shape[2]]);
                    case 3:
                        return sliceAlongLastAxis(array, start, size);
                    default:
                        throw new errors_1.ValueError("The axis is not within the rank of the tensor " +
                            ("" + axis));
                }
            case 4:
                switch (axis) {
                    case 1:
                        return sliceAlongFirstAxis(array, start, size);
                    case 2:
                        return tfc.slice4d(array, [0, start, 0, 0], [array.shape[0], size, array.shape[2], array.shape[3]]);
                    case 3:
                        return tfc.slice4d(array, [0, 0, start, 0], [array.shape[0], array.shape[1], size, array.shape[3]]);
                    case 4:
                        return sliceAlongLastAxis(array, start, size);
                    default:
                        throw new errors_1.ValueError("The axis is not within the rank of the tensor " +
                            ("" + axis));
                }
            default:
                throw new errors_1.ValueError("sliceAlongLastAxis() received an unsupported tensor rank: " +
                    ("" + array.rank));
        }
    });
}
exports.sliceAlongAxis = sliceAlongAxis;
/**
 * Concatenates a list of tensors alongside the specified axis.
 * @param tensors `Array` of tensors to concatenate.
 * @param axis Concatenation axis.
 * @returns The result of the concatenation.
 */
function concatenate(tensors, axis) {
    if (axis === void 0) { axis = -1; }
    var rank;
    if (axis < 0) {
        rank = tensors[0].rank;
        if (rank !== 0) {
            axis = rank;
        }
        else {
            axis = 0;
        }
    }
    if (axis === tensors[0].rank) {
        // Porting Note: This is necessary because tfc.concat() requires axis to be
        //   in the interval [-rank, rank).
        axis = -1;
    }
    // Porting Note: Sparse concat is not supported yet.
    return tfc.concat(tensors, axis);
}
exports.concatenate = concatenate;
/**
 * Concatenate two arrays along the first dimension.
 * @param a The 1st `tf.Tensor` to concatenate.
 * @param b The 2nd `tf.Tensor` to concatenate.
 * @returns Result of the concatenation.
 * @throws ValueError: If `a` is of an unsupported subtype of `tf.Tensor`.
 */
function concatAlongFirstAxis(a, b) {
    switch (a.rank) {
        case 1:
            return tfc.concat1d([a, b]);
        case 2:
            return tfc.concat2d([a, b], 0);
        case 3:
            return tfc.concat3d([a, b], 0);
        case 4:
            return tfc.concat4d([a, b], 0);
        default:
            throw new errors_1.ValueError("concatAlongFirstAxis() received an unsupported " +
                ("tensor rank: " + a.rank));
    }
}
exports.concatAlongFirstAxis = concatAlongFirstAxis;
/**
 * Creates a tensor by tiling `x` by `n`.
 * @param x A tensor.
 * @param n An Array of integers or a single integer. If an Array, the length
 *   must be the same as the number of dimensions in `x`. If a single integer,
 *   it will be treated as an Array of length 1.
 */
function tile(x, n) {
    if (!Array.isArray(n)) {
        n = [n];
    }
    if (x.rank !== n.length) {
        throw new errors_1.ValueError("The length of input n (" + n.length + ") does not match " +
            ("the number of dimensions in input x (" + x.rank + ")"));
    }
    return tfc.tile(x, n);
}
exports.tile = tile;
/* Creation of random tensors. */
/**
 * Get a tensor with normal distribution of values.
 *
 * @param shape Shape of the tensor.
 * @param mean mean value of the normal distribution.
 * @param stddev standard deviation of the normal distribution.
 * @param dtype
 * @param seed
 * @return The normal tensor.
 */
function randomNormal(shape, mean, stddev, dtype, seed) {
    if (mean === void 0) { mean = 0.0; }
    if (stddev === void 0) { stddev = 1.0; }
    return tfc.randomNormal(shape, mean, stddev, dtype, seed);
}
exports.randomNormal = randomNormal;
/* Linear Algebra */
/**
 * Multiply two tensors and returns the result as a tensor.
 *
 * For 2D tensors, this is equivalent to matrix multiplication (matMul).
 * For tensors of higher ranks, it follows the Theano behavior,
 * (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`).  From the Theano documentation:
 *
 * For N dimensions it is a sum product over the last axis of x and the
 * second-to-last of y:
 *
 * @param a A tensor of at least rank 2.
 * @param b A tensor of at least rank 2.
 * @param activation (optional) A string identifying the activation
 *   function.
 * @return Result of the dot operation.
 */
function dot(a, b, activation, bias) {
    if ((a.rank < 2) || (b.rank < 2)) {
        throw new errors_1.NotImplementedError("dot requires both inputs to be rank >= 2" +
            (" but got x shape = " + a.shape + " and y shape = " + b.shape));
    }
    if (b.rank >= 3) {
        var xLastDim = a.shape.slice(-1)[0];
        var ySecondLastDim = b.shape.slice(-2)[0];
        if (xLastDim !== ySecondLastDim) {
            throw new errors_1.NotImplementedError("If rank y >= 3, then the second last dim" +
                (" of y must equal the last dim of x but got x shape = " + a.shape + " and ") +
                (" y shape = " + b.shape));
        }
    }
    // Handle basic 2D x 2D case.
    if ((a.rank === 2) && (b.rank === 2)) {
        var transposeA = false;
        var transposeB = false;
        // tfc.fused.matMul only fuses certain activation functions. Unsupported
        // activation functions are treated as 'linear' activations, which is
        // equivalent to a no-op.
        return tfc.fused.matMul({
            a: a,
            b: b,
            transposeA: transposeA,
            transposeB: transposeB,
            bias: bias ? reshapeBias(a.rank, bias, common_2.imageDataFormat()) : null,
            activation: activation
        });
    }
    else {
        // Reshape x into the analogous 2D Tensor.
        var aFirstDims = a.shape.slice(); // Holds all but the last dim of x.
        var aLastDim = aFirstDims.pop();
        a = a.reshape([-1, aLastDim]);
        // Reshape y into the analogous 2D Tensor, and keep track of the
        // required dimensions to reproduce the output shape.
        var bShape = b.shape.slice();
        var bLastDim = bShape.pop();
        var ySecondLastDim = bShape.pop();
        var yOtherDims = bShape.concat([bLastDim]);
        // permutation should be like [r-2, 0, 1, 2, ... r-4, r-3, r-1]
        // where r is the rank of y.
        var perm = Array.from({ length: b.rank }, function (_, i) {
            if (i === 0) {
                return b.rank - 2;
            }
            else if (i <= b.rank - 2) {
                return i - 1;
            }
            return i;
        });
        b = b.transpose(perm).reshape([ySecondLastDim, -1]);
        // Multiply x and y as 2D Tensors, and then reshape back to original.
        var outputShape = aFirstDims.concat(yOtherDims);
        var transposeA = false;
        var transposeB = false;
        return tfc.fused
            .matMul({
            a: a,
            b: b,
            transposeA: transposeA,
            transposeB: transposeB,
            bias: bias ? reshapeBias(a.rank, bias, common_2.imageDataFormat()) : null,
            activation: activation
        })
            .reshape(outputShape);
    }
}
exports.dot = dot;
/**
 * Compute the sign Tensor of an input Tensor.
 *
 * Elements of the input `tf.Tensor` that are === 0 are mapped to 0.
 * Elements of the input `tf.Tensor` that are > 0 are mapped to 1.
 * Elements of the input `tf.Tensor` that are < 0 are mapped to -1.
 *
 * @param x Input `tf.Tensor`.
 * @return The sign `tf.Tensor`.
 */
function sign(x) {
    // TODO(cais): Move to the core.
    return tfjs_core_1.tidy(function () {
        var zerosLikeX = tfjs_core_1.zerosLike(x);
        var onesLikeX = tfjs_core_1.onesLike(x);
        return tfjs_core_1.where(tfc.equal(x, zerosLikeX), zerosLikeX, tfjs_core_1.where(tfc.greater(x, tfjs_core_1.zerosLike(x)), onesLikeX, tfc.mul(-1, onesLikeX)));
    });
}
exports.sign = sign;
/**
 * Computes the one-hot representation of an integer tensor.
 * @param indices nD integer tensor of shape
 *   `(batch_size, dim1, dim2, ... dim(n-1))`
 * @param numClasses Integer, number of classes to consider.
 * @returns (n + 1)D one hot representation of the input
 *   with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)`
 */
function oneHot(indices, numClasses) {
    return tfjs_core_1.tidy(function () {
        if (indices.rank !== 1) {
            throw new Error('Only 1D one-hot tensors are supported in the ' +
                'deeplearn backend, at present.');
        }
        indices = indices.toInt();
        return tfc.oneHot(indices, numClasses).toFloat();
    });
}
exports.oneHot = oneHot;
/* Elementary math functions. */
/**
 * Retrieves the elements of indices `indices` in the tensor `reference`.
 * @param reference A tensor.
 * @param indices An integer tensor of indices or an `Array` of integers.
 * @param axis Axis along which to perform the gather operation.
 * @returns The result of the gathering as a tensor.
 */
function gather(reference, indices, axis) {
    return tfjs_core_1.tidy(function () {
        if (Array.isArray(indices)) {
            indices = tfjs_core_1.tensor1d(indices, 'int32');
        }
        else {
            indices = indices.toInt();
        }
        return tfc.gather(reference, indices, axis);
    });
}
exports.gather = gather;
/**
 * Element-wise square.
 * @param x Input tensor.
 * @return element-wise x^2
 */
function square(x) {
    return tfc.mulStrict(x, x);
}
exports.square = square;
/**
 * Element-wise exponentiation.
 *
 * Porting Note: In PyKeras, `a` (the exponent) is a Python integer, which
 *   takes advatnage of the backend's (e.g., TensorFlow's) automatic
 * conversion to tensor. Here we allow `a` to be either a number or a tensor.
 *
 * @param x The base tensor.
 * @param a The exponent, tensor or number. If a number, it is rounded to the
 *   nearest integer and converted to a tensor.
 * @returns A tensor of the same shape as `x`.
 */
function pow(x, a) {
    return tfjs_core_1.tidy(function () {
        if (typeof (a) === 'number') {
            a = tfjs_core_1.scalar(Math.round(a), 'int32');
        }
        if (a.dtype !== 'int32') {
            throw new errors_1.NotImplementedError("Non-int32 dtype (" + a.dtype + ") is not supported by pow() yet");
        }
        return tfc.pow(x, a);
    });
}
exports.pow = pow;
/**
 * Reshapes bias tensor according to rank of x.
 */
function reshapeBias(xRank, bias, dataFormat) {
    var biasShape = bias.shape;
    if (bias.rank !== 1 && bias.rank !== xRank) {
        throw new errors_1.ValueError("Unexpected bias dimensions: " + bias.rank +
            ("; expected it to be 1 or " + xRank));
    }
    if (xRank === 5) {
        if (dataFormat === 'channelsFirst') {
            if (biasShape.length === 1) {
                return bias.reshape([1, biasShape[0], 1, 1, 1]);
            }
            else {
                return bias.reshape([1, biasShape[3], biasShape[0], biasShape[1], biasShape[2]]);
            }
        }
        else if (dataFormat === 'channelsLast') {
            if (biasShape.length === 1) {
                return bias.reshape([1, 1, 1, 1, biasShape[0]]);
            }
            else {
                return bias.reshape([1].concat(biasShape));
            }
        }
    }
    else if (xRank === 4) {
        if (dataFormat === 'channelsFirst') {
            if (biasShape.length === 1) {
                return bias.reshape([1, biasShape[0], 1, 1]);
            }
            else {
                return bias.reshape([1, biasShape[2], biasShape[0], biasShape[1]]);
            }
        }
        else if (dataFormat === 'channelsLast') {
            if (biasShape.length === 1) {
                return bias.reshape([1, 1, 1, biasShape[0]]);
            }
            else {
                return bias.reshape([1].concat(biasShape));
            }
        }
    }
    else if (xRank === 3) {
        if (dataFormat === 'channelsFirst') {
            if (biasShape.length === 1) {
                return bias.reshape([1, biasShape[0], 1]);
            }
            else {
                return bias.reshape([1, biasShape[1], biasShape[0]]);
            }
        }
        else if (dataFormat === 'channelsLast') {
            if (biasShape.length === 1) {
                return bias.reshape([1, 1, biasShape[0]]);
            }
            else {
                return bias.reshape([1].concat(biasShape));
            }
        }
    }
    else if (xRank < 3) {
        return bias;
    }
    throw new errors_1.ValueError("Unsupported input rank by biasAdd: " + bias.rank);
}
/* Neural-network operations. */
/**
 * Add a bias to a tensor.
 *
 * @param x The tensor to add the bias to.
 * @param bias The bias to add to `x`. Must be 1D or the same rank as `x`.
 * @return Result of the bias adding.
 * @throws ValueError: If the rank of `bias` is incorrect.
 */
function biasAdd(x, bias, dataFormat) {
    return tfjs_core_1.tidy(function () {
        if (dataFormat == null) {
            dataFormat = common_2.imageDataFormat();
        }
        common_1.checkDataFormat(dataFormat);
        return x.add(reshapeBias(x.rank, bias, dataFormat));
    });
}
exports.biasAdd = biasAdd;
/**
 * Exponential linear unit (ELU).
 * @param x A tensor or variable to compute the activation function for.
 * @param alpha: A scalar, a scaling factor for the negative section.
 * @return Output of the ELU operation.
 */
function elu(x, alpha) {
    if (alpha === void 0) { alpha = 1; }
    // TODO(cais): Add support for alpha values other than 1.
    if (alpha !== 1) {
        throw new errors_1.NotImplementedError("Support for alpha values other than 1 (" + alpha + ") is not implemented " +
            "yet.");
    }
    return tfc.elu(x);
}
exports.elu = elu;
/**
 * Softsign of a tensor.
 *
 * Defined as x / (abs(x) + 1), element-wise.
 *
 * @param x: Input.
 * @returns Output.
 */
function softsign(x) {
    return tfjs_core_1.tidy(function () { return tfc.div(x, tfc.abs(x).add(1)); });
}
exports.softsign = softsign;
/**
 * Sets entries in `x` to zero at random, while scaling the entire tensor.
 *
 * @param x input tensor.
 * @param level fraction of the entries in the tensor that will be set to 0.
 * @param noiseShape shape of randomly generated keep/drop flags, must be
 *   broadcastable to the shape of `x`. Optional.
 * @param seed random seed to ensure determinism. Optional.
 * @returns Result of the dropout operation.
 */
function dropout(x, level, noiseShape, seed) {
    return tfjs_core_1.tidy(function () { return tfc.dropout(x, level, noiseShape, seed); });
}
exports.dropout = dropout;
/**
 * Element-wise, segment-wise linear approximation of sigmoid.
 *
 * Returns `0.` if `x < -2.5`, `1.` if `x > 2.5`.
 * In `-2.5 <= x <= 2.5`, returns `0.2 * x + 0.5`.
 *
 * @param x Input tensor.
 * @returns Output tensor.
 */
function hardSigmoid(x) {
    return tfjs_core_1.tidy(function () {
        var y = tfc.add(.5, tfc.mul(.2, x));
        return tfc.clipByValue(y, 0, 1);
    });
}
exports.hardSigmoid = hardSigmoid;
/**
 * Invoke `x` in the training phase, and `alt` otherwise.
 *
 * Porting Note: We do not create placeholder tensors for the `training`
 * boolean flag here, because there is no such thing in the TF.js imperative
 * backend.
 *
 * @param x The function to invoke iff `training` is `true`.
 * @param alt The function to invoke iff `training` is `false`.
 * @param training Boolean flag for whether training phase is active.
 * @returns The return value of `x()` if `training` is `true`, or the return
 *   value of `alt()` if `training` is `false`.
 */
function inTrainPhase(x, alt, training) {
    if (training === void 0) { training = false; }
    return training ? x() : alt();
}
exports.inTrainPhase = inTrainPhase;
//# sourceMappingURL=tfjs_backend.js.map